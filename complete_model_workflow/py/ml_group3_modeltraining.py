# -*- coding: utf-8 -*-
"""ML_Group3_ModelTraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j4xcLeYh9Wbr4P2dcJjOYT7YCR9cyQhP
"""

from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Path to your file
file_path = '/content/drive/MyDrive/DATA245-Machine-Learning-Group(3)/ML_Project/email_job_scam_cleaned.csv'

# Read the CSV file
df = pd.read_csv(file_path)

df = pd.read_csv(file_path)
df = df.drop(columns=['job_id'])

df.head()

"""#1. Decision Tree"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
from sklearn import tree
# Preprocessing
# Handle categorical variables - we'll treat all object/string columns as categorical
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Label encoding for categorical variables
label_encoders = {}
for col in categorical_cols:
    if col in df.columns:
        # Fill missing values with 'Unknown' before encoding
        df[col] = df[col].fillna('Unknown')
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

# Fill remaining numerical NA values with 0 (or you could use mean/median)
df = df.fillna(0)

# Define features and target
X = df.drop('is_scam', axis=1)
y = df['is_scam']

# Split the data into training (70%) and test (30%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42, max_depth=5)  # Limiting depth for interpretability
dt_model.fit(X_train, y_train)

# Make predictions
y_pred = dt_model.predict(X_test)

# Evaluate the model
print("\nModel Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Visualize the decision tree (might be large)
#plt.figure(figsize=(20,10))
#tree.plot_tree(dt_model, feature_names=X.columns, class_names=['Not Scam', 'Scam'], filled=True, rounded=True, proportion=True)
#plt.title("Decision Tree for Job Scam Prediction")
#plt.show()

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': dt_model.feature_importances_
}).sort_values('Importance', ascending=False)

#print("\nFeature Importance:")
#print(feature_importance)

cm_dt = confusion_matrix(y_test, y_pred)

"""#2. Random Forest"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns


# Initialize and train the Random Forest model
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42,
    class_weight='balanced'
)

rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature Importance Visualization
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

#plt.figure(figsize=(8, 4))
#sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))
#plt.title('Top 15 Important Features for Scam Detection')
#plt.tight_layout()
#plt.show()

cm_rf = confusion_matrix(y_test, y_pred)

"""#3. Naive Baye's"""

from sklearn.naive_bayes import GaussianNB

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

cm_nb = confusion_matrix(y_test, y_pred)

"""#4. Logistic Regression"""

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)

y_pred = logreg.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

cm_lr = confusion_matrix(y_test, y_pred)

"""#5. RBF - SVM"""

from sklearn.svm import SVC

svm_rbf_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_rbf_model.fit(X_train_scaled, y_train)

y_pred = svm_rbf_model.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

cm_svm_rbf = confusion_matrix(y_test, y_pred)

"""#6. Linear SVM"""

svm_linear_model = SVC(kernel='linear', probability=True, random_state=42)
svm_linear_model.fit(X_train_scaled, y_train)

y_pred = svm_linear_model.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

cm_svm_linear = confusion_matrix(y_test, y_pred)

"""#7. Polynomial SVM"""

svm_poly_model = SVC(kernel='poly', probability=True, random_state=42)
svm_poly_model.fit(X_train_scaled, y_train)

y_pred = svm_poly_model.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

cm_svm_poly = confusion_matrix(y_test, y_pred)

"""#8. kNN"""

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)  # k is tunable
knn_model.fit(X_train_scaled, y_train)

y_pred = knn_model.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

cm_knn = confusion_matrix(y_test, y_pred)

"""#9. Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Use a simple decision tree as the base estimator (can tune depth)
base_estimator = DecisionTreeClassifier(max_depth=1)

# Initialize AdaBoost
ada_model = AdaBoostClassifier(
    estimator=base_estimator,
    n_estimators=100,
    learning_rate=1.0,
    random_state=42
)

# Fit model
ada_model.fit(X_train_scaled, y_train)

# Predict
y_pred_ada = ada_model.predict(X_test_scaled)

# Evaluation
print("AdaBoost Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_ada):.4f}")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_ada))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_ada))

cm_ada = confusion_matrix(y_test, y_pred_ada)

"""#10. XGBoost"""

from xgboost import XGBClassifier
xgb_model = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

xgb_model.fit(X_train_scaled, y_train)

y_pred_xgb = xgb_model.predict(X_test_scaled)

print("XGBoost Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

cm_xgb = confusion_matrix(y_test, y_pred_xgb)

"""#SHAP"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a TF-IDF vectorizer and transform the text data into numeric features.
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()
X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()

# Extract the feature names from the vectorizer
feature_names = tfidf_vectorizer.get_feature_names_out()


background_data = X_train_tfidf[:100]   # Use a small subset for efficiency (adjust as needed)
explanation_data = X_test_tfidf[:100]     # Subset of test data for computing SHAP values

import shap
import matplotlib.pyplot as plt

# Assume the following models dictionary is defined and contains your trained models:
models = {
    'Decision Tree': dt_model,
    'Random Forest': rf_model,
    'Naive Bayes': nb,
    'Logistic Regression': logreg,
    'SVM RBF': svm_rbf_model,
    'SVM Linear': svm_linear_model,
    'SVM Poly': svm_poly_model,
    'KNN': knn_model,
    'AdaBoost': ada_model,
    'XGBoost': xgb_model
}

# Identify which of these models are tree-based
# Note: We remove 'AdaBoost' from tree_models because scikit-learn's AdaBoostClassifier is not supported by TreeExplainer.
tree_models = ['Decision Tree', 'Random Forest', 'XGBoost']

# Loop through all models and generate SHAP explanations
for model_name, model in models.items():
    print("\nGenerating SHAP values for model:", model_name)

    try:
        # Use TreeExplainer for tree-based models if applicable
        if model_name in tree_models:
            explainer = shap.TreeExplainer(model)
        else:
            # For non-tree models or unsupported tree-based models (like AdaBoost), use KernelExplainer
            explainer = shap.KernelExplainer(model.predict, background_data)

        # Compute SHAP values for the explanation dataset
        shap_values = explainer.shap_values(explanation_data)

        # Create a summary plot for the model
        plt.title("SHAP Summary for " + model_name)
        shap.summary_plot(shap_values, explanation_data, feature_names=feature_names, show=True)

    except Exception as e:
        print(f"Error computing SHAP for {model_name}: {e}")

"""

---



#KPIs"""

from sklearn.metrics import roc_curve, auc

# Store the models
models = {
    'Decision Tree': dt_model,
    'Random Forest': rf_model,
    'Naive Bayes': nb,
    'Logistic Regression': logreg,
    'SVM RBF': svm_rbf_model,
    'SVM Linear': svm_linear_model,
    'SVM Poly': svm_poly_model,
    'KNN': knn_model,
    'AdaBoost': ada_model,
    'XGBoost': xgb_model
}

# Plot settings
plt.figure(figsize=(7, 6))

for model_name, model in models.items():
    # Use scaled or unscaled test data depending on the model
    if model_name in ['Logistic Regression', 'SVM RBF', 'SVM Linear', 'SVM Poly', 'KNN']:
        X_input = X_test_scaled
    else:
        X_input = X_test

    # Get probability or decision function
    try:
        if hasattr(model, "predict_proba"):
            probas_ = model.predict_proba(X_input)[:, 1]
        elif hasattr(model, "decision_function"):
            # Normalize decision_function output to [0, 1] for ROC
            decision_scores = model.decision_function(X_input)
            probas_ = (decision_scores - decision_scores.min()) / (decision_scores.max() - decision_scores.min())
        else:
            continue  # Skip models that can't produce a score
    except Exception as e:
        print(f"Error for {model_name}: {e}")
        continue

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, probas_)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')

# Plot diagonal reference line
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

# Final plot touches
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""---
#Fine-tuning
"""

from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 20],
    'min_samples_split': [5, 10],
    'class_weight': ['balanced']
}

grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf,
                       cv=5, scoring='f1', n_jobs=-1, verbose=2)
grid_rf.fit(X_train, y_train)

print("Best Params:", grid_rf.best_params_)
print("Best F1 Score:", grid_rf.best_score_)
cm_grid_rf = confusion_matrix(y_test, grid_rf.predict(X_test))

from xgboost import XGBClassifier

param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [5, 7],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8],
    'colsample_bytree': [0.8],
    'gamma': [0, 0.1]
}


xgb_clf = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

grid_xgb = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid_xgb,
    cv=5,
    scoring='f1',  # or 'roc_auc', 'accuracy', etc.
    n_jobs=-1,
    verbose=2
)

grid_xgb.fit(X_train, y_train)

cm_grid_xgb = confusion_matrix(y_test, grid_xgb.predict(X_test))
print("Best Params:", grid_xgb.best_params_)
print("Best F1 Score:", grid_xgb.best_score_)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Param grid for AdaBoost
param_grid_ada = {
    'n_estimators': [100, 200],
    'learning_rate': [0.5, 1.0],
    'estimator': [
        DecisionTreeClassifier(max_depth=1),
        DecisionTreeClassifier(max_depth=3)
    ]
}

# Initialize GridSearchCV
grid_ada = GridSearchCV(
    estimator=AdaBoostClassifier(random_state=42),
    param_grid=param_grid_ada,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid_ada.fit(X_train, y_train)

cm_grid_ada = confusion_matrix(y_test, grid_ada.predict(X_test))

print("Best AdaBoost Params:", grid_ada.best_params_)
print("Best F1 Score:", grid_ada.best_score_)

models = {
    'Random Forest': rf_model,
    'AdaBoost': ada_model,
    'XGBoost': xgb_model,
    'Random Forest Tuned': grid_rf,
    'AdaBoost Tuned': grid_ada,
    'XGBoost Tuned': grid_xgb

}

plt.figure(figsize=(7, 6))

for model_name, model in models.items():
    if model_name in ['Logistic Regression', 'SVM RBF', 'SVM Linear', 'SVM Poly', 'KNN']:
        X_input = X_test_scaled
    else:
        X_input = X_test

    try:
        if hasattr(model, "predict_proba"):
            probas_ = model.predict_proba(X_input)[:, 1]
        elif hasattr(model, "decision_function"):
            decision_scores = model.decision_function(X_input)
            probas_ = (decision_scores - decision_scores.min()) / (decision_scores.max() - decision_scores.min())
        else:
            continue
    except Exception as e:
        print(f"Error for {model_name}: {e}")
        continue

    fpr, tpr, _ = roc_curve(y_test, probas_)
    roc_auc = auc(fpr, tpr)

    if model_name in ['Random Forest', 'XGBoost', 'AdaBoost']:
        color = 'grey'
    else:
        color = None

    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})', color=color)

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

from sklearn.ensemble import VotingClassifier


voting_clf = VotingClassifier(
    estimators=[
        ('rf', grid_rf),
        ('xgb', grid_xgb),
        ('svm_rbf', svm_rbf_model),
        ('svm_linear', svm_linear_model),
        ('svm_poly', svm_poly_model),
        ('knn', knn_model),
        ('ada', grid_ada),
        ('dt', dt_model),
        ('nb', nb),
        ('lr', logreg)
    ],
    voting='soft'  # Use 'soft' because your models can predict probabilities
)

voting_clf.fit(X_train_scaled, y_train)

y_pred_voting = voting_clf.predict(X_test_scaled)

voting_accuracy = accuracy_score(y_test, y_pred_voting)
print(f"Voting Classifier Accuracy: {voting_accuracy:.4f}")

cm_voting = confusion_matrix(y_test, y_pred_voting)
print(cm_voting)

models = {
    'Random Forest Tuned': grid_rf,
    'Voting Classifier': voting_clf
}

plt.figure(figsize=(7, 6))

for model_name, model in models.items():
    if model_name in ['Logistic Regression', 'SVM RBF', 'SVM Linear', 'SVM Poly', 'KNN', 'Voting Classifier']:
        X_input = X_test_scaled
    else:
        X_input = X_test

    try:
        if hasattr(model, "predict_proba"):
            probas_ = model.predict_proba(X_input)[:, 1]
        elif hasattr(model, "decision_function"):
            decision_scores = model.decision_function(X_input)
            probas_ = (decision_scores - decision_scores.min()) / (decision_scores.max() - decision_scores.min())
        else:
            continue
    except Exception as e:
        print(f"Error for {model_name}: {e}")
        continue

    fpr, tpr, _ = roc_curve(y_test, probas_)
    roc_auc = auc(fpr, tpr)

    if model_name in ['Random Forest', 'XGBoost', 'AdaBoost', 'AdaBoost Tuned', 'XGBoost Tuned']:
        color = 'grey'
    else:
        color = None

    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})', color=color)

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    fbeta_score, matthews_corrcoef, cohen_kappa_score, confusion_matrix
)
import numpy as np
import pandas as pd

# Function to calculate all metrics from a confusion matrix
def evaluate_from_confusion_matrix(cm, average='binary'):
    TP = cm[1, 1]
    TN = cm[0, 0]
    FP = cm[0, 1]
    FN = cm[1, 0]

    y_true = np.array([1]*TP + [0]*TN + [0]*FP + [1]*FN)
    y_pred = np.array([1]*TP + [0]*TN + [1]*FP + [0]*FN)

    return {
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, average=average, zero_division=0),
        'Recall': recall_score(y_true, y_pred, average=average, zero_division=0),
        'F1 Score': f1_score(y_true, y_pred, average=average, zero_division=0),
        'F0.5 Score': fbeta_score(y_true, y_pred, beta=0.5, average=average, zero_division=0),
        'F2 Score': fbeta_score(y_true, y_pred, beta=2.0, average=average, zero_division=0),
        'MCC': matthews_corrcoef(y_true, y_pred),
        'Kappa': cohen_kappa_score(y_true, y_pred)
    }

# All confusion matrices; add XGBoost's confusion matrix (cm_xgb) here:
conf_matrices = {
    "Decision Tree": cm_dt,
    "Random Forest": cm_grid_rf,
    "Naive Bayes": cm_nb,
    "Logistic Regression": cm_lr,
    "SVM RBF": cm_svm_rbf,
    "SVM Linear": cm_svm_linear,
    "SVM Poly": cm_svm_poly,
    "KNN": cm_knn,
    "AdaBoost": cm_grid_ada,
    "XGBoost": cm_grid_xgb,
    "Voting Classifier": cm_voting
}

# Evaluate and compile results into a DataFrame
results = {model: evaluate_from_confusion_matrix(cm) for model, cm in conf_matrices.items()}
results_df = pd.DataFrame(results).T.round(4)

print(results_df)

"""#Test with an example"""

from math import nan
import pandas as pd
from sklearn.impute import KNNImputer

new_row_data = {
    'title': 7050,
    'location': 1642,
    'department': 812,
    'salary_range': 950,
    'telecommuting': 0,
    'has_company_logo': 1,
    'has_questions': 1,
    'employment_type': 1,
    'required_experience': 3,
    'required_education': 4,
    'industry': 120,
    'function': 30,
    'non_https_links': 0.0,
    'id_info_requested': 0.0,
    'avg_days_offer': 4,
    'avg_urgent_terms': 0,
    'avg_bait_phrases': 4
}

imputer = KNNImputer(n_neighbors=5)
X_train_imputed = imputer.fit_transform(X_train)


X_test_imputed = imputer.transform(X_test)
new_row_imputed = imputer.transform(X_test_imputed)

new_test_df = pd.DataFrame([new_row_data])

row_index = 0

#print(new_test_df)
test_row_scaled = scaler.transform(new_test_df)

predicted_class = grid_rf.predict(new_test_df)[0]
predicted_proba = rf_model.predict_proba(new_test_df)[0]

print(f"Predicted class: {predicted_class}")
print(f"Predicted probabilities: Legitimate = {predicted_proba[0]:.4f}, Fraudulent = {predicted_proba[1]:.4f}")

"""# Export Model"""

import joblib

# Save the GridSearchCV-tuned Random Forest model
joblib.dump(grid_rf, 'grid_rf_model.pkl')

print("Model saved as grid_rf_model.pkl")

